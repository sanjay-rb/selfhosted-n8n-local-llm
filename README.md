# selfhosted-n8n-local-llm

Self-hosted **n8n + Ollama (Llama3.2) + AnimateDiff Video Generation** stack for **8GB M1/M2 Macs**.

**One-command AI automation + text-to-video generation!** ğŸ¥ğŸ¤–

## ğŸš€ Quick Start

```bash
# 1. Clone & setup
git clone <your-repo>
cd selfhosted-n8n-local-llm
mkdir videogen
# Copy app.py (see below)

# 2. Start stack
docker compose up -d

# 3. Install Ollama model (one-time)
docker exec ollama ollama pull llama3.2:3b

# 4. Wait for video model (~10min first run)
docker logs -f videogen
# Wait for: "âœ… AnimateDiff pipeline LOADED SUCCESSFULLY!"

# 5. Access:
# n8n: http://localhost:5678
# Ollama API: http://localhost:11434
# VideoGen API: http://localhost:8000
```

## ğŸ“ Folder Structure

```
selfhosted-n8n-local-llm/
â”œâ”€â”€ docker-compose.yml      # Complete stack
â”œâ”€â”€ videogen/
â”‚   â””â”€â”€ app.py             # AnimateDiff video API
â”œâ”€â”€ README.md              # This file
â””â”€â”€ n8n_data/              # Persistent (auto-created)
```

## ğŸ§  **Required: videogen/app.py**

Create `./videogen/app.py`:
```python
from fastapi import FastAPI
from diffusers import MotionAdapter, AnimateDiffPipeline, StableDiffusionPipeline
from diffusers.utils import export_to_video
import torch
from pydantic import BaseModel

app = FastAPI()

print("ğŸš€ Loading AnimateDiff...")
try:
    adapter = MotionAdapter.from_pretrained("guoyww/animatediff-motion-adapter-v1-5-2")
    pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float32, safety_checker=None)
    pipeline = AnimateDiffPipeline.from_pipe(pipe, motion_adapter=adapter)
    pipeline.to("cpu")
    pipeline.enable_vae_slicing()
    app.state = type('State', (), {'pipe': pipeline, 'loaded': True})()
    print("âœ… AnimateDiff LOADED!")
except Exception as e:
    print(f"âŒ Load failed: {e}")
    app.state = type('State', (), {'pipe': None, 'loaded': False})()

class GenRequest(BaseModel):
    prompt: str = "astronaut riding horse"
    num_frames: int = 16

@app.post("/generate")
async def generate(req: GenRequest):
    if not app.state.loaded: return {"error": "Model not ready"}
    output = app.state.pipe(req.prompt, num_frames=16, num_inference_steps=25, guidance_scale=7.5)
    return {"status": "success", "frames": len(output.frames[0])}

@app.get("/health")
async def health():
    return {"status": "ready" if app.state.loaded else "loading", "model": "AnimateDiff"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

## ğŸ¯ **Usage Examples**

### **1. Test Video Generation**
```bash
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "cat dancing on moon, cinematic"}'
```

### **2. n8n Workflow: Ollama â†’ Video**
```
Ollama Chat â†’ HTTP Request (videogen)
Prompt: "Describe epic video scene"
â†’ POST http://videogen:8000/generate
   Body: {"prompt": "{{ $('Ollama').item.json.response }}"}
```

### **3. n8n Ollama Settings**
```
Ollama Chat Model Node:
âœ… Base URL: http://ollama:11434
âœ… Model: llama3.2:3b (EXACT TAG)
```

## ğŸ“Š **Services**

| Service | Port | Purpose | First Run |
|---------|------|---------|-----------|
| **n8n** | 5678 | Workflow automation | Instant |
| **Ollama** | 11434 | Llama3.2:3b (2GB) | 2-5min download |
| **VideoGen** | 8000 | AnimateDiff AI video | 10min model download |

## ğŸ’¾ **Persistent Storage**
```
n8n_data/          # Workflows
ollama_data/       # Llama3.2 model (2GB+)
videogen_data/     # HuggingFace cache (4GB+)
```

## âš™ï¸ **Hardware Requirements**
- âœ… **8GB RAM M1/M2 Mac** (tested)
- âœ… **16GB+** = faster video generation
- âŒ **<8GB** = OOM errors

## â±ï¸ **Performance (8GB M1 Mac)**
```
Video generation: 16 frames = 3-8 minutes
Ollama response: 10-60 seconds
n8n workflows: Instant
RAM usage: ~5-6GB peak
```

## ğŸ”§ **Troubleshooting**

### **VideoGen stuck downloading?**
```bash
docker logs videogen  # Normal warnings OK
# Wait for: "âœ… AnimateDiff LOADED!"
curl http://localhost:8000/health  # {"status": "ready"}
```

### **Ollama "model not found"?**
```bash
docker exec ollama ollama pull llama3.2:3b
# In n8n: use EXACT "llama3.2:3b" (no :latest)
```

### **n8n can't reach services?**
```
Use http://ollama:11434 (not localhost)
Use http://videogen:8000 (not localhost)
```

### **Clean restart:**
```bash
docker compose down -v
docker compose up -d
docker exec ollama ollama pull llama3.2:3b
```

## ğŸ¨ **Example n8n Workflow**

```
1. Cron Trigger (daily)
2. Ollama Chat: "Suggest viral video idea"
3. HTTP Request â†’ videogen:8000/generate
4. Email/Slack: "Video ready: {{$json.video_url}}"
```

## ğŸ“± **Access URLs**
```
n8n UI: http://localhost:5678
Video API: http://localhost:8000/health
Ollama API: http://localhost:11434/api/tags
```

## ğŸš€ **Production Ready**
- âœ… Persistent storage
- âœ… Auto-restart
- âœ… London timezone
- âœ… n8n Runners enabled
- âœ… 8GB RAM optimized